"""
è¿‡æ»¤CSVæ–‡ä»¶ä¸­é‡å¤çš„title
å¯¹äºæ¯ä¸ªURLï¼Œæ¯ä¸ªtitleåªä¿ç•™ç¬¬ä¸€ä¸ªå‡ºç°çš„è®°å½•
"""

import csv
import os
from collections import defaultdict
from datetime import datetime


def filter_duplicate_titles(input_file: str, output_file: str = None):
    """è¿‡æ»¤é‡å¤çš„titleï¼Œæ¯ä¸ªURLä¸­æ¯ä¸ªtitleåªä¿ç•™ç¬¬ä¸€ä¸ª
    
    Args:
        input_file: è¾“å…¥CSVæ–‡ä»¶è·¯å¾„
        output_file: è¾“å‡ºCSVæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
    """
    
    if not os.path.exists(input_file):
        print(f"é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
        return
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šè¾“å‡ºæ–‡ä»¶ï¼Œè‡ªåŠ¨ç”Ÿæˆ
    if output_file is None:
        base_name = os.path.basename(input_file).replace(".csv", "")
        dir_name = os.path.dirname(input_file)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = os.path.join(dir_name, f"{base_name}_filtered_{timestamp}.csv")
    
    # ç”¨äºè·Ÿè¸ªæ¯ä¸ª (url, title) ç»„åˆæ˜¯å¦å·²å‡ºç°
    seen_combinations = set()
    unique_rows = []
    duplicate_count = 0
    total_count = 0
    
    # è¯»å–CSVæ–‡ä»¶
    print(f"æ­£åœ¨è¯»å–æ–‡ä»¶: {input_file}")
    with open(input_file, 'r', encoding='utf-8-sig') as f:
        reader = csv.DictReader(f)
        fieldnames = reader.fieldnames
        
        if 'title' not in fieldnames or 'url' not in fieldnames:
            print("é”™è¯¯: CSVæ–‡ä»¶å¿…é¡»åŒ…å« 'title' å’Œ 'url' åˆ—")
            return
        
        for row in reader:
            total_count += 1
            title = row.get('title', '').strip()
            url = row.get('url', '').strip()
            
            # åˆ›å»ºå”¯ä¸€æ ‡è¯† (url, title)
            combination = (url, title)
            
            # å¦‚æœè¿™ä¸ªç»„åˆè¿˜æ²¡å‡ºç°è¿‡ï¼Œä¿ç•™è¿™ä¸€è¡Œ
            if combination not in seen_combinations:
                seen_combinations.add(combination)
                unique_rows.append(row)
            else:
                duplicate_count += 1
    
    # å†™å…¥æ–°çš„CSVæ–‡ä»¶
    print(f"æ­£åœ¨å†™å…¥è¿‡æ»¤åçš„æ–‡ä»¶: {output_file}")
    with open(output_file, 'w', encoding='utf-8-sig', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(unique_rows)
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print(f"\nâœ… è¿‡æ»¤å®Œæˆï¼")
    print(f"   åŸå§‹è®°å½•æ•°: {total_count}")
    print(f"   å»é‡åè®°å½•æ•°: {len(unique_rows)}")
    print(f"   åˆ é™¤é‡å¤è®°å½•æ•°: {duplicate_count}")
    print(f"   ä¿ç•™ç‡: {len(unique_rows)/total_count*100:.2f}%")
    print(f"   è¾“å‡ºæ–‡ä»¶: {output_file}")
    
    # ç»Ÿè®¡æ¯ä¸ªURLçš„å»é‡æƒ…å†µ
    url_stats = defaultdict(lambda: {'total': 0, 'unique': 0})
    for row in unique_rows:
        url = row.get('url', '').strip()
        url_stats[url]['unique'] += 1
    
    # é‡æ–°è¯»å–åŸå§‹æ–‡ä»¶ç»Ÿè®¡æ¯ä¸ªURLçš„æ€»æ•°
    with open(input_file, 'r', encoding='utf-8-sig') as f:
        reader = csv.DictReader(f)
        for row in reader:
            url = row.get('url', '').strip()
            url_stats[url]['total'] += 1
    
    print(f"\nğŸ“Š URLç»Ÿè®¡ä¿¡æ¯ï¼ˆå‰10ä¸ªï¼‰:")
    sorted_urls = sorted(url_stats.items(), key=lambda x: x[1]['total'], reverse=True)
    for url, stats in sorted_urls[:10]:
        print(f"   {url[:60]}...")
        print(f"      åŸå§‹: {stats['total']}, å»é‡å: {stats['unique']}, åˆ é™¤: {stats['total'] - stats['unique']}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='è¿‡æ»¤CSVæ–‡ä»¶ä¸­é‡å¤çš„titleï¼ˆæ¯ä¸ªURLä¸­æ¯ä¸ªtitleåªä¿ç•™ç¬¬ä¸€ä¸ªï¼‰')
    parser.add_argument('input_file', help='è¾“å…¥CSVæ–‡ä»¶è·¯å¾„')
    parser.add_argument('-o', '--output', help='è¾“å‡ºCSVæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä¸ºè¾“å…¥æ–‡ä»¶å_filtered_æ—¶é—´æˆ³.csvï¼‰')
    
    args = parser.parse_args()
    
    filter_duplicate_titles(args.input_file, args.output)


if __name__ == "__main__":
    main()

